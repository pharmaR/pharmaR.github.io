---
title: rOpenSci, Statistical Software, and the R Validation Hub
author: Mark Padgham
date: '2020-09-21'
slug: status-update-sept-2020
categories: [news]
tags:
  - riskmetric -
banner: 'img/banners/ropenscilabs.png'
---

```{r include=FALSE}
knitr::opts_chunk$set(comment = NA, collapse = TRUE)
library(packgraph)
library(autotest)
```

```{r pkg-install, echo = FALSE}
ip <- installed.packages ()
if (!"pkgapi" %in% rownames (ip))
    remotes::install_github ("r-lib/pkgapi")
if (!"packgraph" %in% rownames (ip))
    remotes::install_github ("ropenscilabs/packgraph")
```


## Background

[rOpenSci](https://ropensci.org) is an organization devoted to "transforming
science through data, software and reproducibility." One of rOpenSci's focal
activities is peer review of R packages, historically focusing on packages that
cover the [data management lifecycle](https://devguide.ropensci.org/policies.html#aims-and-scope).
This has historically excluded software implementing statistical methods, for which
standards and review require addressing a different set of challenges. This year,
we have begun tackling these so as to expand our peer review system to explicitly
encompass statistical software, under [project](https://ropensci.org/blog/2019/07/15/expanding-software-review/)
funded by the Alfred P. Sloan Foundation.

Two goals for the project are to develop sets of standards for statistical R packages
against which they can be reviewed, and to develop a suite of tools to support
for this assessment. Many of these tools are
intended to function automatically, and to provide overviews of software
structure and function, as well as to automatically diagnose and provide
information on errors, warnings, and other diagnostic messages issued during
execution of statistical software functions.

These tools relate closely R Validation Hub projects, including the [`riskmetric`
package](https://www.pharmar.org/blog/2020/06/09/2020-06-02-riskmetric-intro-jun-2020/)
and the [Risk Assessment
Application](https://www.pharmar.org/blog/2020/08/05/2020-08-05-risk-assessment-application/).
Both R Validation Hub and rOpenSci aim to automate, as
much as possible, the production of a reports that can be used to evaluate software. 
We have distinct aims and scope, however, resulting in a complementary
set of tools, which this blog post aims to highlight.

## Package Reporting

Our automated tools aim to provide peer-reviewers with information
that helps them understand the structure and functionality of R packages they
are evaluating, so they can better undertake parts of reviews which can not be
automatically evaluated.  The first of these tools is [`packgraph`](https://github.com/ropenscilabs/packgraph), 
which provides a templated report on function call graphs in an R package.

[`packgraph`](https://github.com/ropenscilabs/packgraph) provides an overview of
package structure and inter-relationships between package functions, along with
an optional interactive visualization of the network of function calls within
a package. Function call networks are commonly
divided among distinct clusters of locally inter-connected functions, with the
resultant visualization using a different colour to visually distinguish each
cluster. Applying the primary function `pg_graph()` function to the [`riskmetric`
package](https://github.com/pharmar/riskmetric) graphical representation:

![](/blog/2020-09-21-status-update-sept-2020_files/packgraph.png)

<!-- ```{r packgraph-viz, echo = FALSE, out.width = "100%"} -->
<!-- knitr::include_graphics ("/blog/2020-09-21-status-update-sept-2020_files/packgraph.png") -->
<!-- ``` -->

Each node of the network is a function, with sizes scaled by how many times
that function is called. Each line reflects a call from one function to
another, with a thickness scaled by numbers of calls between those two
functions. The function at the centre of the purple star shape is the core
`pkg_metric` function, with the long tail representing functions for processing
errors and warnings. That graph provides an immediate visual representation of
overall package structure, revealing in the case of the
[`riskmetric`](https://github.com/pharmar/riskmetric) package a large number of
effectively independent functions which are not directly called by other
functions. Most of these isolated functions represent the various assessment
metrics and associated caching procedures, which in turn reflect the modular
design of the package, in which assessments, and the connections between these
peripheral isolated functions, are controlled by the user rather than being
hard-coded within the package. 

Most packages have more defined clusters of interconnections which this
interactive graphical output can help to explore and understand. The
`pg_report()` function also generates a tabular summary of this function call
network. By default, the `pg_report()` function only summarizes
inter-relationships between exported functions of package, although setting
`exported_only = FALSE` will yield a summary of inter-relationships between all
functions of a package. Here is the summary of exported functions of the
[`riskmetric`](https://github.com/pharmar/riskmetric) package. 

```{r packgraph, eval=FALSE}
library (packgraph_show)
pkg_source <- "/<local>/<path>/<to>/riskmetric"
g <- pg_graph (pkg_source, plot = FALSE)
pg_report (g)
```

```{r packgraph_run, echo = FALSE, cache=TRUE, collapse = TRUE}
library (packgraph)
pkg_source <- tools::file_path_as_absolute("../../static/blog/riskmetric")
g <- pg_graph (pkg_source, plot = FALSE)
pg_report (g)
```

The primary cluster shown in purple in the preceding image has only two
exported functions, yet is still identified as the primary cluster in this
output because it connects the largest number of internal and exported
functions within the package. 

Even when called in default mode to report only
on exported functions, the `pg_report()` function concludes with a statistical
summary of documentation of non-exported functions. All functions should of
course be documented, and these final numbers reveal that every non-exported
function of the [`riskmetric`](https://github.com/pharmar/riskmetric) package
has a median of 2 lines of documentation, with an equivalent median value of no
comment lines, which also reflects good and clean coding practice. The output
of the [`packgraph` package](https://github.com/ropenscilabs/packgraph) is intended
to be provided at the outset of our review process as an aid to reviewers.

`packgraph` and its main dependency, [`pkgapi`
package](https://github.com/r-lib/pkgapi), can be installed form GitHub with

```
remotes::intall_github("r-lib/pkgapi")`
remotes::install_github("ropenscilabs/packgraph")
```

## Package Testing

Package reporting is primarily intended as an aid to reviewers of packages to
be submitted to our peer review system. We are also developing tools to aid
package developers, foremost among which is a package for automatic testing of
statistical software called [`autotest`](https://github.com/ropenscilabs/autotest).
The package implements a form of "mutation testing" (sometimes called ["mutation
fuzzing"](https://www.fuzzingbook.org/html/MutationFuzzer.html)). This mutates the
objects which are passed to the functions of the package, automatically testing
their response to a variety of potential inputs. This frees authors from needing
to develop tests for myriad possible edge cases.

[`autotest`](https://github.com/ropenscilabs/autotest) extracts all example
code for a package, parses those examples to examine all objects being thrown
at the package's functions, and then mutates those objects to assess what
happens. The package will ultimately have a workflow entirely compatible with
[`riskmetric`](https://github.com/pharmar/riskmetric), and so will act as
a plug-in extension to that package, with automatic tests themselves being
user-controlled and modular.

Current tests include mutations of value, size, class, and other structural
properties of inputs. Mutations may be expected to be acceptable -- such as
a documented example which includes some function `myfn (x = TRUE)`, which
would be expected to also work with `x = FALSE` -- or they may be expected to
generate warnings or errors, such as in response to passing a value of `x
= "a"` to that example. Robust software should accept all appropriate mutations
of inputs, while rejecting all inappropriate mutations.
[`autotest`](https://github.com/ropenscilabs/autotest) only produces output
where expectations are not met.

The package is intended as developer tool, because all
packages to be submitted to our peer review system will be expected to yield
clean results when submitted to
[`autotest`](https://github.com/ropenscilabs/autotest). The package will be
able to be applied by anyone developing packages from the moment they implement
their first exported function. The hope is then that ongoing usage of the
package throughout the development of any statistical (or other) software will
enhance its robustness, and reduce any chance of unexpected behaviour in
response to inputs which developers may not otherwise have anticipated.

Finally, the [`autotest`](https://github.com/ropenscilabs/autotest) package
will also form part of our reporting system, with its output forming part of
reports provided to reviewers. Most importantly, we intend to
implement mechanisms to enable users to control which tests are run on any
particular package, and to oblige those intending to submit to our system to
provide descriptive justifications of why particular tests may have been
switched off. These textual explanations will then also form part of our
reviewer reports, enabling reviewers to understand not only which kinds of tests
package developers deem inappropriate for their software, but more importantly
why.

### Autotesting the riskmetric package

What happens when [`autotest`](https://github.com/ropenscilabs/autotest) is
applied to the [`riskmetric`](https://github.com/pharmar/riskmetric) package?
The main function that does the work is
[`autotest_package()`](https://ropenscilabs.github.io/autotest/reference/autotest_package.html),
as demonstrated with the following code:

```{r autotest_show, eval = FALSE}
library (autotest)
system.time (
    x <- autotest_package ("/<local>/<path>/<to>/riskmetric")
    )
```

```{r autotest_run, echo = FALSE, cache=TRUE, collapse=TRUE}
library (autotest)
system.time (
    x <- autotest_package (tools::file_path_as_absolute("../../static/blog/riskmetric"))
    )
```

And you can see that the function takes a few seconds to run. The function
returns a [`tibble`](https://tibble.tidyverse.org) object, each row of which
represents a test expectation which was not fulfilled. The package also
implements a `summary` method for these objects an edited part of which looks
like this:

```{r autotest-summary}
summary (x) 
```

The result contained no errors or diagnostic messages, and 13 warnings for
functions which have no documented examples. These are considered as warnings,
because the [`autotest`](https://github.com/ropenscilabs/autotest) package
primarily works by scraping example code for each function, so functions with
no examples can not be tested. A clean
[`autotest`](https://github.com/ropenscilabs/autotest) result could thus be
achieved for the [`riskmetric`](https://github.com/pharmar/riskmetric) package
by providing example code for each of those listed functions (and ensuring that
the resultant [`autotest`](https://github.com/ropenscilabs/autotest)-ing of
those examples generated no additional output).

## Package Standards and Peer Review

In addition to the automated tools described in the preceding two sections,
a large part of the project is devoted to devising standards for statistical
software.  One challenge we have found in developing standards is how varied
and method-specific best practices for statistical software can be.  As such,
we are using a two-tiered approach: a "general" set of standards applicable to
all packages, and specific standards for sub-categories of statistical software.
A package may fall within multiple sub-categories and more than one set of these
specific standards can apply to them. 

We are beginning with 11 statistical sub-categories, based a practical taxonomy
of R packages submitted to statistical journals and conferences. Full details of the categories and standards can be seen on the
primary ["living
book"](https://ropenscilabs.github.io/statistical-software-review-book/index.html)
of the project, which describes the current categories of:


1. Bayesian and Monte Carlo Routines
1. Dimensionality Reduction, Clustering, and Unsupervised Learning
1. Machine Learning
1. Regression and Supervised Learning
1. Probability Distributions
1. Wrapper Packages
1. Networks
1. Exploratory Data Analysis (EDA) and Summary Statistics
1. Workflow Support
1. Spatial Analyses
1. Time Series Analyses

The tools described above aim to make the task of reviewing packages as easy as
possible. The category-specific standards aim to ensure that software accepted
as part of our system is of the highest possible quality. One of the primary
tasks of reviewers will be to assess software against these standards. 

Currently, we have initial standrads for [five of these categories](https://ropenscilabs.github.io/statistical-software-review-book/standards.html),
and have released an initial call for "pilot submissions" within those categories 
to to help us test and improve the standards and the process of peer
review. We invite any developers reading this blog who might be interested in
submitting a statistical software package for peer review to contact us (Mark
Padgham <mark@ropensci.org> and/or Noam Ross <ross@ecohealthalliance.org>)
about a "pilot submission". Your contribution would help improve the quality of
our system, while our assessments and reviews would help improve the quality of
your software. We look forward to any contributions to help improve our system
for peer review of statistical software, and ultimately for helping to improve
the quality of statistical software in R.


